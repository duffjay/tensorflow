{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2\n",
    "#### MNIST - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8cd6ef07f1ab>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# deprecatioin warnings\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  # flattened images 28x28 as a vector\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "#### Summary Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variable_summary(tf_variable, summary_name):\n",
    "    with tf.name_scope(summary_name + '_summary'):\n",
    "        # calc mean\n",
    "        mean = tf.reduce_mean(tf_variable)\n",
    "        tf.summary.scalar('Mean', mean)\n",
    "        # calc std dev\n",
    "        with tf.name_scope('standard_deviation'):\n",
    "            standard_deviation = tf.sqrt(tf.reduce_mean(tf.square(tf_variable - mean)))\n",
    "            \n",
    "        tf.summary.scalar('StandardDeviation', standard_deviation)\n",
    "        tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
    "        tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
    "        tf.summary.histogram('Histogram', tf_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "shape each MNIST digit as 28x28x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape operation\n",
    "x_input_reshape = tf.reshape(x_input, [-1,28,28,1], name='input_reshape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer Function\n",
    "return a convolution layer  \n",
    "include a tensorboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(input_layer, filters, kernel_size=[3,3], activation=tf.nn.relu):\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation=activation)\n",
    "    add_variable_summary(layer, 'convolution')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer Function\n",
    "return a pooling layer  \n",
    "include a tensorboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_layer(input_layer, pool_size=[2,2], strides=2):\n",
    "    layer = tf.layers.max_pooling2d(\n",
    "        inputs=input_layer,\n",
    "        pool_size=pool_size,\n",
    "        strides=strides)\n",
    "    add_variable_summary(layer, 'pooling')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Function\n",
    "input - a single dimension (flat) vector  \n",
    " a hidden layer (units), 1024 in this example  \n",
    "output reLU activation   \n",
    "return a dense layer  \n",
    "\n",
    "include a tensorboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(input_layer,units, activation=tf.nn.relu):\n",
    "    layer = tf.layers.dense(\n",
    "        inputs=input_layer,\n",
    "        units=units,\n",
    "        activation=activation)\n",
    "    add_variable_summary(layer, 'dense')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "convolution_layer_1 = convolution_layer(x_input_reshape, 64)   # 64 filters\n",
    "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
    "\n",
    "# layer 2\n",
    "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)  # 128 filters\n",
    "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
    "\n",
    "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5*5*128], name = 'flattened_pool')    # n, 5x5, filters\n",
    "\n",
    "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Out\n",
    "layer 2 -> boolean x Dropout -> Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_bool = tf.placeholder(tf.bool)  # placeholder\n",
    "dropout_layer = tf.layers.dropout(\n",
    "    inputs=dense_layer_bottleneck,\n",
    "    rate=0.4,\n",
    "    training=dropout_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = dense_layer(dropout_layer, no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-6ab4e7252a03>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_input, logits=logits)\n",
    "    loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
    "    tf.summary.scalar('loss', loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy_operation = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32))\n",
    "        \n",
    "tf.summary.scalar('accuracy', accuracy_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session & Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Writer\n",
    "the graph is written once  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary_operation = tf.summary.merge_all()\n",
    "train_summary_writer = tf.summary.FileWriter('/tmp/train', session.graph)\n",
    "test_summary_writer  = tf.summary.FileWriter('/tmp/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Images (100, 784)\n",
      "train_labels (100, 10)\n",
      "batch# 0\n",
      "batch# 1\n",
      "batch# 2\n",
      "batch# 3\n",
      "batch# 4\n",
      "batch# 5\n",
      "batch# 6\n",
      "batch# 7\n",
      "batch# 8\n",
      "batch# 9\n",
      "batch# 10\n",
      "batch# 11\n",
      "batch# 12\n",
      "batch# 13\n",
      "batch# 14\n",
      "batch# 15\n",
      "batch# 16\n",
      "batch# 17\n",
      "batch# 18\n",
      "batch# 19\n",
      "batch# 20\n",
      "batch# 21\n",
      "batch# 22\n",
      "batch# 23\n",
      "batch# 24\n",
      "batch# 25\n",
      "batch# 26\n",
      "batch# 27\n",
      "batch# 28\n",
      "batch# 29\n",
      "batch# 30\n",
      "batch# 31\n",
      "batch# 32\n",
      "batch# 33\n",
      "batch# 34\n",
      "batch# 35\n",
      "batch# 36\n",
      "batch# 37\n",
      "batch# 38\n",
      "batch# 39\n",
      "batch# 40\n",
      "batch# 41\n",
      "batch# 42\n",
      "batch# 43\n",
      "batch# 44\n",
      "batch# 45\n",
      "batch# 46\n",
      "batch# 47\n",
      "batch# 48\n",
      "batch# 49\n",
      "batch# 50\n",
      "batch# 51\n",
      "batch# 52\n",
      "batch# 53\n",
      "batch# 54\n",
      "batch# 55\n",
      "batch# 56\n",
      "batch# 57\n",
      "batch# 58\n",
      "batch# 59\n",
      "batch# 60\n",
      "batch# 61\n",
      "batch# 62\n",
      "batch# 63\n",
      "batch# 64\n",
      "batch# 65\n",
      "batch# 66\n",
      "batch# 67\n",
      "batch# 68\n",
      "batch# 69\n",
      "batch# 70\n",
      "batch# 71\n",
      "batch# 72\n",
      "batch# 73\n",
      "batch# 74\n",
      "batch# 75\n",
      "batch# 76\n",
      "batch# 77\n",
      "batch# 78\n",
      "batch# 79\n",
      "batch# 80\n",
      "batch# 81\n",
      "batch# 82\n",
      "batch# 83\n",
      "batch# 84\n",
      "batch# 85\n",
      "batch# 86\n",
      "batch# 87\n",
      "batch# 88\n",
      "batch# 89\n",
      "batch# 90\n",
      "batch# 91\n",
      "batch# 92\n",
      "batch# 93\n",
      "batch# 94\n",
      "batch# 95\n",
      "batch# 96\n",
      "batch# 97\n",
      "batch# 98\n",
      "batch# 99\n",
      "batch# 100\n",
      "batch# 101\n",
      "batch# 102\n",
      "batch# 103\n",
      "batch# 104\n",
      "batch# 105\n",
      "batch# 106\n",
      "batch# 107\n",
      "batch# 108\n",
      "batch# 109\n",
      "batch# 110\n",
      "batch# 111\n",
      "batch# 112\n",
      "batch# 113\n",
      "batch# 114\n",
      "batch# 115\n",
      "batch# 116\n",
      "batch# 117\n",
      "batch# 118\n",
      "batch# 119\n",
      "batch# 120\n",
      "batch# 121\n",
      "batch# 122\n",
      "batch# 123\n",
      "batch# 124\n",
      "batch# 125\n",
      "batch# 126\n",
      "batch# 127\n",
      "batch# 128\n",
      "batch# 129\n",
      "batch# 130\n",
      "batch# 131\n",
      "batch# 132\n",
      "batch# 133\n",
      "batch# 134\n",
      "batch# 135\n",
      "batch# 136\n",
      "batch# 137\n",
      "batch# 138\n",
      "batch# 139\n",
      "batch# 140\n",
      "batch# 141\n",
      "batch# 142\n",
      "batch# 143\n",
      "batch# 144\n",
      "batch# 145\n",
      "batch# 146\n",
      "batch# 147\n",
      "batch# 148\n",
      "batch# 149\n",
      "batch# 150\n",
      "batch# 151\n",
      "batch# 152\n",
      "batch# 153\n",
      "batch# 154\n",
      "batch# 155\n",
      "batch# 156\n",
      "batch# 157\n",
      "batch# 158\n",
      "batch# 159\n",
      "batch# 160\n",
      "batch# 161\n",
      "batch# 162\n",
      "batch# 163\n",
      "batch# 164\n",
      "batch# 165\n",
      "batch# 166\n",
      "batch# 167\n",
      "batch# 168\n",
      "batch# 169\n",
      "batch# 170\n",
      "batch# 171\n",
      "batch# 172\n",
      "batch# 173\n",
      "batch# 174\n",
      "batch# 175\n",
      "batch# 176\n",
      "batch# 177\n",
      "batch# 178\n",
      "batch# 179\n",
      "batch# 180\n",
      "batch# 181\n",
      "batch# 182\n",
      "batch# 183\n",
      "batch# 184\n",
      "batch# 185\n",
      "batch# 186\n",
      "batch# 187\n",
      "batch# 188\n",
      "batch# 189\n",
      "batch# 190\n",
      "batch# 191\n",
      "batch# 192\n",
      "batch# 193\n",
      "batch# 194\n",
      "batch# 195\n",
      "batch# 196\n",
      "batch# 197\n",
      "batch# 198\n",
      "batch# 199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch_no in range(total_batches):\n",
    "    # retreive a batch\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    if batch_no == 0:\n",
    "        print (\"train_Images\", train_images.shape)\n",
    "        print (\"train_labels\", train_labels.shape)\n",
    "    print (\"batch#\", batch_no)\n",
    "    # run the batch through the model\n",
    "    #   merged_summary is the outut values of the optimizer\n",
    "    _, merged_summary = session.run([optimizer, merged_summary_operation], feed_dict={\n",
    "        x_input: train_images,\n",
    "        y_input: train_labels,\n",
    "        dropout_bool: True\n",
    "    })\n",
    "    # summary_writer\n",
    "    train_summary_writer.add_summary(merged_summary, batch_no)\n",
    "    # add test statistics every 10th loop\n",
    "    if batch_no % 10 == 0:\n",
    "        merged_summary, _ = session.run([merged_summary_operation, accuracy_operation], feed_dict={\n",
    "            x_input: test_images,\n",
    "            y_input: test_labels,\n",
    "            dropout_bool: False\n",
    "        })\n",
    "        test_summary_writer.add_summary(merged_summary, batch_no)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
